# Technical Test: Backend Software Developer

This project was developed as part of a technical test to implement an HTTP API for searching URLs containing a term provided by the user. The application uses a hexagonal architecture, allowing greater testability and maintainability.

---

## ğŸ¯ **Objective**

Develop a Java application to navigate a website and list URLs containing a term provided by the user. The system supports multiple simultaneous searches and returns partial results while the search is in progress.

---

## ğŸš€ **How to Run the Application**

### **Prerequisites**

- **Docker** installed on your machine
- **Environment variable** `BASE_URL` configured to determine the base URL for link searches.


### **Steps to Start the Application**

1. Build the Docker image:
```bash
docker build . -t webcrawler/backend
```

2. Run the Docker container:
```bash
docker run -e BASE_URL=https://man7.org/linux/man-pages/ -p 4567:4567 --rm webcrawler/backend
```

The application will be available at: `http://localhost:4567`.

---

## ğŸ“š **Project Structure**

The application follows the hexagonal architecture:

```
com.webcrawler.backend/
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ model/             
â”‚   â”‚   â”œâ”€â”€ CrawlJob.java         # Represents the characteristics of a search job
â”‚   â”‚   â”œâ”€â”€ CrawlStatus.java      # Enum for status ("active" or "done")
â”‚   â”œâ”€â”€ port/                    
â”‚       â”œâ”€â”€ CrawlerService.java   # Port for executing crawling
â”‚       â”œâ”€â”€ CrawlRepository.java  # Port for job persistence
â”œâ”€â”€ application/
â”‚   â”œâ”€â”€ CrawlServiceImpl.java     # Implements domain use cases
â”œâ”€â”€ adapter/
â”‚   â”œâ”€â”€ in/
â”‚   â”‚   â”œâ”€â”€ web/
â”‚   â”‚   â”‚   â”œâ”€â”€ CrawlController.java # Controls API routes
â”‚   â”‚   â”‚   â”œâ”€â”€ dto/
â”‚   â”‚   â”‚       â”œâ”€â”€ CrawlRequest.java  # Representation of POST request
â”‚   â”‚   â”‚       â”œâ”€â”€ CrawlResponse.java # Representation of GET response
â”‚   â”œâ”€â”€ out/
â”‚       â”œâ”€â”€ crawler/
â”‚       â”‚   â”œâ”€â”€ WebCrawlerImpl.java    # Search mechanism implementation
â”‚       â”œâ”€â”€ persistence/
â”‚           â”œâ”€â”€ InMemoryCrawlRepository.java # In-memory search repository
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ SparkConfig.java             # Spark server configuration
â”œâ”€â”€ Main.java                        # Application entry point
â””â”€â”€ test/
    â”œâ”€â”€ domain/
    â”‚   â”œâ”€â”€ CrawlJobTest.java        # Unit tests for domain model
    â”œâ”€â”€ application/
    â”‚   â”œâ”€â”€ CrawlServiceImplTest.java # Unit tests for use cases
    â”œâ”€â”€ adapter/
    â”‚   â”œâ”€â”€ in/
    â”‚   â”‚   â”œâ”€â”€ web/
    â”‚   â”‚       â”œâ”€â”€ CrawlControllerTest.java # Tests for API controller
    â”‚   â”œâ”€â”€ out/
    â”‚       â”œâ”€â”€ crawler/
    â”‚           â”œâ”€â”€ WebCrawlerImplTest.java # Tests for search mechanism
```

---

## âœ¨ **Main Features**

1. **POST `/crawl`**:
    - Starts a new search with the provided term.
    - **Request**:
```json
{
  "keyword": "security"
}
```

    - **Response**:
    ```json
{
"id": "abc12345"
}
```

2. **GET `/crawl/:id`**:
    - Returns the search status and URLs found so far.
    - **Response**:
```json
{
  "id": "abc12345",
  "status": "active",
  "urls": [
    "https://man7.org/linux/man-pages/man7/security.7.html",
    "https://man7.org/linux/man-pages/"
  ]
}
```

3. Support for **multiple simultaneous searches**.
4. Return of **partial results** during search execution.
5. **Parallel execution** controlled by environment variable (`MULTITHREAD_OPT`).

---

## ğŸ”§ **Technologies Used**

- **Java 14**: Main language for implementation.
- **Maven**: Dependency and build manager.
- **Spark Java**: Framework for HTTP API.
- **Gson**: Library for JSON serialization/deserialization.
- **JUnit 5**: Framework for unit testing.
- **Mockito**: Mocking library to simulate dependencies in tests.

---

## âš™ï¸ **Environment Variables**

- `BASE_URL`: Defines the base URL where searches will be performed.
    - Default value: `https://man7.org/linux/man-pages/`
- `MULTITHREAD_OPT`: Defines whether the search will be performed in parallel.
    - Accepted values: `"true"` or `"false"`
    - Default value: `"false"`

---

## âœ… **Implemented Tests**

Tests cover all layers of the application:

1. **Domain (`CrawlJobTest`)**:
    - Validation of job creation and manipulation (e.g., adding found URLs).
2. **Application (`CrawlServiceImplTest`)**:
    - Tests use cases for starting searches and querying existing jobs.
3. **Input Adapters (API, `CrawlControllerTest`)**:
    - Validates expected behavior of POST `/crawl` and GET `/crawl/:id` endpoints.
4. **Output Adapters (`InMemoryCrawlRepositoryTest`)**:
    - Validates in-memory repository, ensuring correct persistence.
5. **WebCrawler (`WebCrawlerImplTest`)**:
    - Tests URL resolution logic and link extraction.

---

## ğŸ“Š **Performance Evaluation**

- Optimized parallel search using `ExecutorService`.
- Visited URLs are stored in a thread-safe structure (`ConcurrentHashMap` or `synchronizedSet`).
- Limit of 100 results per search, as specified in the test.

---

## ğŸ“ **Notes**

1. The files `pom.xml` and `Dockerfile` were **not modified**, as requested in the technical test.
2. The application meets all functional requirements described, including:
    - Validation of `keyword` (minimum 4, maximum 32 characters).
    - Respecting the base URL, following only links within the same domain.
    - Partial result return via API while the search is still in progress.
3. The solution is designed to be testable, scalable, and easy to maintain.

---

ğŸ’¡ **Questions or suggestions? Feel free to reach out! ğŸ˜Š

---

## **Future Updates**

- Implement a file system cache to store search results.
- Fix tests (I didn't have time to fix them all).
- Add more unit tests to cover all edge cases.
- Implement a more robust error handling mechanism.

---